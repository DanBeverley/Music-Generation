{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1246998,"sourceType":"datasetVersion","datasetId":716027},{"sourceId":10588873,"sourceType":"datasetVersion","datasetId":6553319}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install miditoolkit\n!pip install torchtoolkit\n!pip install pretty_midi\n!pip install miditok\n\nimport os\nimport json\nimport torch\nimport logging\nimport pandas as pd\nimport random\nfrom pretty_midi import PrettyMIDI\n\nfrom pathlib import Path\nfrom typing import Callable, Any, Dict, List, Optional\nfrom miditoolkit import MidiFile, Instrument, Note\nfrom miditok import REMI, TokenizerConfig\nfrom torchtoolkit.data import create_subsets\nfrom tqdm import tqdm\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import LongTensor\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.optim as optim\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:45:04.775447Z","iopub.execute_input":"2025-01-28T08:45:04.775729Z","iopub.status.idle":"2025-01-28T08:45:18.214428Z","shell.execute_reply.started":"2025-01-28T08:45:04.775709Z","shell.execute_reply":"2025-01-28T08:45:18.213390Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: miditoolkit in /usr/local/lib/python3.10/dist-packages (1.0.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from miditoolkit) (3.7.1)\nRequirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.10/dist-packages (from miditoolkit) (1.3.3)\nRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from miditoolkit) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mido>=1.1.16->miditoolkit) (24.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->miditoolkit) (1.3.0)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->miditoolkit) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->miditoolkit) (4.53.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->miditoolkit) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->miditoolkit) (10.4.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->miditoolkit) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->miditoolkit) (2.8.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->miditoolkit) (1.16.0)\nRequirement already satisfied: torchtoolkit in /usr/local/lib/python3.10/dist-packages (0.0.4)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchtoolkit) (2.4.1+cu121)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from torchtoolkit) (1.26.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtoolkit) (4.66.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchtoolkit) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchtoolkit) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchtoolkit) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchtoolkit) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchtoolkit) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchtoolkit) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchtoolkit) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchtoolkit) (1.3.0)\nRequirement already satisfied: pretty_midi in /usr/local/lib/python3.10/dist-packages (0.2.10)\nRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.26.4)\nRequirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.3.3)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.16.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mido>=1.1.16->pretty_midi) (24.1)\nRequirement already satisfied: miditok in /usr/local/lib/python3.10/dist-packages (3.0.4)\nRequirement already satisfied: huggingface-hub>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from miditok) (0.24.7)\nRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from miditok) (1.26.4)\nRequirement already satisfied: symusic>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from miditok) (0.5.6)\nRequirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from miditok) (0.19.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from miditok) (4.66.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->miditok) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->miditok) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->miditok) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->miditok) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->miditok) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->miditok) (4.12.2)\nRequirement already satisfied: pySmartDL in /usr/local/lib/python3.10/dist-packages (from symusic>=0.5.0->miditok) (1.3.4)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from symusic>=0.5.0->miditok) (4.3.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (2024.8.30)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"class AbstractMusicDataset(Dataset):\n    \"\"\"\n    Abstract base class for music dataset\n\n    Args:\n        data_path (List[Path]) : Path to the dataset\n        preprocess_fn (Callable): Function to preprocess a single sample\n        max_seq_len (int) : Maximum sequence length of the data\n        pad_token (int) : token used for padding sequences\n    \"\"\"\n    def __init__(self, data_path:List[Path],\n                preprocess_fn:Callable, max_seq_len:int,\n                pad_token:int, **kwargs):\n        self.data_path = data_path\n        self.preprocess_fn = preprocess_fn\n        self.max_seq_len = max_seq_len\n        self.pad_token = pad_token\n        self.kwargs = kwargs\n        self.data = self.load_data(**kwargs)\n    def load_data(self, **kwargs)->List[Any]:\n        \"\"\"Load any data from specified data path, Override this in subclasses\"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method\")\n\n    def preprocess(self, sample:Any)->Dict[str,Any]:\n        \"\"\"Preprocess a single data sample\"\"\"\n        return self.preprocess_fn(sample, max_seq_len=self.max_seq_len,\n                                 pad_token=self.pad_token)\n    def __len__(self)->int:\n        return len(self.data)\n    def __getitem__(self, idx:int)->Dict[str,Any]:\n        return self.preprocess(self.data[idx])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:40:22.709945Z","iopub.execute_input":"2025-01-28T08:40:22.710317Z","iopub.status.idle":"2025-01-28T08:40:22.716418Z","shell.execute_reply.started":"2025-01-28T08:40:22.710288Z","shell.execute_reply":"2025-01-28T08:40:22.715602Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class MIDIDataset(AbstractMusicDataset):\n    def __init__(self, data_path:List[Path],preprocess_fn:Callable,\n                 max_seq_len:int, pad_token:int):\n        super().__init__(data_path, preprocess_fn,\n                         max_seq_len, pad_token)\n    def load_data(self)->List[MidiFile]:\n        \"\"\"Load MIDI file from dataset directory\"\"\"\n        try:\n            files = list(self.data_path.glob(\"*.midi\"))\n            if not files:\n                logger.warning(f\"No MIDI files found in: {self.data_path}\")\n                return []\n            return [MidiFile(str(file)) for file in files]\n        except FileNotFoundError:\n            logger.error(f\"MIDI directory not found: {self.data_path}\")\n            return []\n        except Exception as e:\n            logger.error(f\"Error loading MIDI files: {e}\")\n            return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:40:22.717949Z","iopub.execute_input":"2025-01-28T08:40:22.718163Z","iopub.status.idle":"2025-01-28T08:40:22.741204Z","shell.execute_reply.started":"2025-01-28T08:40:22.718144Z","shell.execute_reply":"2025-01-28T08:40:22.740341Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class CSVDataset(AbstractMusicDataset):\n    def __init__(self, data_path:List[Path],preprocess_fn:Callable,\n                 csv_filename:str, max_seq_len:int, pad_token:int):\n        super().__init__(data_path, preprocess_fn, max_seq_len, pad_token,\n                        csv_filename = csv_filename)\n        self.csv_filename = csv_filename\n    def load_data(self, csv_filename:str)->List[Dict[str, Any]]:\n        \"\"\"Load the CSV file into a list of dictionaries\"\"\"\n        csv_path = self.data_path/self.csv_filename # Use Path object for joining\n        try:\n            df = pd.read_csv(str(csv_path)) \n            return df.to_dict(orient=\"records\")\n        except FileNotFoundError:\n            logger.error(f\"CSV file not found: {csv_path}\")\n            return []\n        except pd.errors.ParseError as e:\n            logger.error(f\"Error parsing CSV file: {e}\")\n            return []\n        except Exception as e:\n            logger.error(f\"An unexpected error occured while loading or parsing CSV file: {e}\")\n            return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:40:22.742420Z","iopub.execute_input":"2025-01-28T08:40:22.742709Z","iopub.status.idle":"2025-01-28T08:40:22.761005Z","shell.execute_reply.started":"2025-01-28T08:40:22.742675Z","shell.execute_reply":"2025-01-28T08:40:22.760297Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class JSONDataset(AbstractMusicDataset):\n    def __init__(self, data_path:List[Path], preprocess_fn:Callable,\n                max_seq_len:int, pad_token:int, json_filename:str):\n        super().__init__(data_path, preprocess_fn, max_seq_len, pad_token,\n                        json_filename = json_filename)\n        self.json_filename = json_filename\n    def load_data(self, json_filename:str)->List[Dict[str, Any]]:\n        \"\"\"Load a JSON file containing the dataset.\"\"\"\n        json_path = self.data_path[0]/self.json_filename\n        try:\n            with open(str(json_path), \"r\") as f:\n                return json.load(f)\n        except FileNotFoundError:\n            logger.error(f\"JSON file not found: {json_path}\")\n            return []\n        except json.JSONDecodeError as e:\n            logger.error(f\"Error decoding JSON: {e}\")\n            return []\n        except Exception as e:\n            logger.error(f\"An unexpected error occured while loading or parsing JSON file: {e}\")\n            return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:40:22.761949Z","iopub.execute_input":"2025-01-28T08:40:22.762238Z","iopub.status.idle":"2025-01-28T08:40:22.776194Z","shell.execute_reply.started":"2025-01-28T08:40:22.762208Z","shell.execute_reply":"2025-01-28T08:40:22.775524Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def midi_preprocess(sample:PrettyMIDI, max_seq_len:int, pad_token:int,\n                   sos_token:int, eos_token:int, tokenizer:REMI\n                    , default_sample=None)->Dict[str, torch.Tensor]:\n    \"\"\"\n    Preprocess a MIDI sample: tokenize, truncate, and pad.\n\n    Args:\n        sample (MidiFile): MIDI file sample.\n        max_seq_len (int): Maximum sequence length.\n        pad_token (int): Padding token.\n    Returns:\n        Dict[str, torch.Tensor]: Preprocessed sample with input IDs and labels.\n    \"\"\"\n    try:\n        # Use REMI tokenizer for full musical representation\n        tokens = tokenizer.midi_to_tokens(sample)\n        tokens = [sos_token] + tokens[:max_seq_len-2] + [eos_token]\n        padded_tokens = torch.nn.functional.pad(\n            torch.tensor(tokens, dtype=torch.long),\n            (0, max_seq_len - len(tokens)),\n            value=pad_token\n        )\n        return {\"input_ids\": padded_tokens, \"labels\": padded_tokens}\n    except Exception as e:\n        logger.error(f\"MIDI preprocessing failed: {e}\")\n        return default_sample\n\ndef csv_preprocess(sample:Dict[str, Any],\n                  max_seq_len:int,\n                  pad_token:int,\n                  sos_token:int,\n                  eos_token:int,\n                  default_sample:Dict[str, torch.Tensor]=None)->Dict[str, torch.Tensor]:\n    \"\"\"\n    Preprocess CSV data for model input.\n\n    Args:\n        sample: A dictionary containing 'notes' as a key with a list of integer tokens.\n        max_seq_len: The maximum sequence length.\n        pad_token: The token used for padding.\n        sos_token: The start-of-sequence token.\n        eos_token: The end-of-sequence token.\n        default_sample: Optional default sample for invalid data.\n\n    Returns:\n        A dictionary with 'input_ids' and 'labels' tensors.\n    \"\"\"\n    try:\n        # For validating input format\n        if not isinstance(sample, dict) or \"notes\" not in sample:\n            raise ValueError(\"Invalid CSV sample format: missing 'notes' key.\")\n        tokens = sample[\"notes\"]\n        # Check token validity\n        if not all(isinstance(token, int) for token in tokens):\n            raise TypeError(\"CSV tokens must be integers.\")\n\n        # Add special tokens and truncats\n        tokens = [sos_token] + tokens[:max_seq_len-2]+[eos_token]\n        # Pad the sequence\n        padded_tokens = torch.nn.functional.pad(torch.tensor(tokens, dtype=torch.long),\n                                               (0, max_seq_len-len(tokens)),\n                                               value=pad_token)\n        logger.info(f\"Processed CSV data: {len(tokens)} tokens (max {max_seq_len}).\")\n        return {\n            \"input_ids\": padded_tokens,\n            \"labels\":padded_tokens\n        }\n    except (ValueError, TypeError) as e:\n        logger.error(f\"Error preprocessing CSV: {e}\")\n        return default_sample or {\n            \"input_ids\":torch.full((max_seq_len), pad_token, dtype=torch.long),\n            \"labels\":torch.full((max_seq_len), pad_token, dtype=torch.long)\n        }\n\ndef json_preprocess(sample:Dict[str, Any],\n                   max_seq_len:int,\n                   pad_token:int,\n                   sos_token:int,\n                   default_sample:Dict[str, torch.Tensor]=None)->Dict[str,torch.Tensor]:\n    \"\"\"\n    Preprocess JSON data for model input.\n\n    Args:\n        sample: A dictionary containing 'sequence' as a key with a list of integer tokens.\n        max_seq_len: The maximum sequence length.\n        pad_token: The token used for padding.\n        sos_token: The start-of-sequence token.\n        eos_token: The end-of-sequence token.\n        default_sample: Optional default sample for invalid data.\n\n    Returns:\n        A dictionary with 'input_ids' and 'labels' tensors.\n    \"\"\"\n    try:\n        # Validate input format\n        if not isinstance(sample, dict) or \"sequence\" not in sample:\n            raise ValueError(\"Invalid JSON sample format: missing 'sequence' key.\")\n        tokens = sample[\"sequence\"]\n        \n        # Check token validity\n        if not all(isinstance(token,int) for token in tokens):\n            raise TypeError(\"JSON tokens must be integers.\")\n        # Add special tokens and truncate\n        tokens = [sos_token]+tokens[:max_seq_len-2]+[eos_token]\n        # Pad the sequence\n        padded_tokens = torch.nn.functional.pad(torch.tensor(tokens, dtype=torch.long),\n                                               (0, max_seq_len-len(tokens)), value=pad_token)\n        logger.info(f\"Processed JSON data: {len(tokens)} tokens (max {max_seq_len}).\")\n        return {\"input_ids\":padded_tokens,\n               \"labels\":padded_tokens}\n    except (ValueError, TypeError) as e:\n        logger.error(f\"Error preprocessing JSON:{e}\")\n        return default_sample or {\"input_ids\":torch.full((max_seq_len),pad_token, dtype=torch.long),\n                                 \"labels\":torch.full((max_seq_len), pad_token, dtype=torch.long)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:40:22.776949Z","iopub.execute_input":"2025-01-28T08:40:22.777171Z","iopub.status.idle":"2025-01-28T08:40:22.796741Z","shell.execute_reply.started":"2025-01-28T08:40:22.777146Z","shell.execute_reply":"2025-01-28T08:40:22.795998Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### **Preprocessing for the specific MaestroV2 Dataset**","metadata":{}},{"cell_type":"code","source":"class MaestroDataset(Dataset):\n    \"\"\"\n    A dataset for processing Maestro MIDI files.\n\n    Args:\n        file_paths (list): List of paths to MIDI or JSON files.\n        min_seq (int): Minimum sequence length.\n        max_seq (int): Maximum sequence length.\n        tokenizer_config (TokenizerConfig, optional): Configuration for the tokenizer (default: None)\n        pad_token (int): The token used for padding sequences.\n        preprocess (bool): Whether to preprocess and save tokenized files.\n        output_dir (Path): Directory to save preprocessed token files.\n\n    Attributes:\n        samples (list): List of tokenized sequences.\n    \"\"\"\n    def __init__(self, file_paths:List[Path],\n                min_seq:int,\n                max_seq:int,\n                pad_token:int,\n                tokenizer_config: TokenizerConfig=None,\n                preprocess:bool=True,\n                output_dir:Path=None):\n        \n        if not isinstance(file_paths, list):\n            file_paths = [file_paths]\n        file_paths = [Path(fp) for fp in file_paths]\n        \n        self.samples = []\n        self.pad_token = pad_token\n        # Preprocessing if needed\n        if preprocess and output_dir is not None:\n            self._preprocessing_(file_paths, tokenizer_config, output_dir)\n            file_paths = list(output_dir.glob(\"*.json\"))\n        # Load preprocessed tokens\n        self.load_samples(file_paths, min_seq, max_seq)\n    def _preprocessing_(self, file_paths:List[Path],\n                       tokenizer_config:TokenizerConfig,\n                       output_dir:Path):\n        try:\n            output_dir = Path(output_dir)\n            output_dir.mkdir(parents=True, exist_ok=True)\n        except Exception as e:\n            logger.error(f\"Failed to create output directory: {e}\")\n            raise\n            \n        for i in tqdm(file_paths, desc=\"Preprocessing MIDI files\"):\n            try:\n                if i.suffix in [\"MIDI\", \"MID\", \"midi\", \"mid\"]:\n                    midi = MidiFile(i)\n                    if tokenizer_config is None:\n                        tokenizer_config = TokenizerConfig()\n                    tokenizer = REMI(tokenizer_config) if tokenizer_config is not None else REMI(tokenizer_config)\n                    all_tracks_tokens = [tokenizer.midi_to_tokens(midi)[0].ids \n                                         for track in midi.tracks if len(track)>0]\n                    tokens = [token for track in all_tracks_tokens for token in track]\n                else:\n                    continue # Skip non-MIDI files\n                # Save tokens to JSON\n                output_file = output_dir/f\"{i.stem}_tokens.json\"\n                with open(output_file, \"w\") as f:\n                    json.dump({\"ids\":tokens}, f)\n            except Exception as e:\n                logger.warning(f\"Error processing {i}: {e}\")\n    def load_samples(self, file_paths:List[Path],\n                        min_seq:int,\n                        max_seq:int):\n        \"\"\"Load tokenized samples and create sequences\"\"\"\n        for file_path in tqdm(file_paths, desc=\"Loading tokenized files\"):\n            try:\n                with open(file_path, \"r\") as f:\n                    tokens = json.load(f)[\"ids\"]\n                # Create fixed length sequences\n                i = 0\n                while i<len(tokens):\n                    if i>=len(tokens)-min_seq:\n                        break\n                    self.samples.append(LongTensor(tokens[i:i+max_seq]))\n                    i+=len(self.samples[-1])\n            except Exception as e:\n                logger.warning(f\"Error loading {file_path}: {e}\")\n    def __getitem__(self, idx)->Dict[str, LongTensor]:\n        return {\"input_ids\":self.samples[idx],\n               \"labels\":self.samples[idx]}\n    def __len__(self)->int:\n        return len(self.samples)\n    def __regr__(self):\n        return self.__str__()\n    def __str__(self)->str:\n        return \"No data loaded\" if len(self)==0 else f\"{len(self.samples)} samples\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:40:22.797589Z","iopub.execute_input":"2025-01-28T08:40:22.797833Z","iopub.status.idle":"2025-01-28T08:40:22.818384Z","shell.execute_reply.started":"2025-01-28T08:40:22.797813Z","shell.execute_reply":"2025-01-28T08:40:22.817551Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def collate_fn(batch:List[Dict[str, LongTensor]],\n              pad_token:int)->Dict[str,LongTensor]:\n    \"\"\"\n    Collate function for dynamic padding.\n    Args:\n        batch: List of dictionaries, each containing 'input_ids' and 'labels'.\n        pad_token: Token used for padding.\n    Returns:\n        A dictionary with padded 'input_ids' and 'labels' as LongTensor.\n    \"\"\"\n    input_ids = [item[\"input_ids\"] for item in batch]\n    labels    = [item[\"labels\"] for item in batch]\n\n    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=pad_token).long()\n    labels = pad_sequence(labels, batch_first=True, padding_value=pad_token).long()\n\n    return {\"input_ids\":input_ids, \"labels\":labels}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:40:22.819175Z","iopub.execute_input":"2025-01-28T08:40:22.819461Z","iopub.status.idle":"2025-01-28T08:40:22.836082Z","shell.execute_reply.started":"2025-01-28T08:40:22.819440Z","shell.execute_reply":"2025-01-28T08:40:22.835450Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# **Model Architecture**\n### 1. Attention Mechanism","metadata":{}},{"cell_type":"code","source":"class RelativeAttention(nn.Module):\n    \"\"\"\n    Relative self-attention mechanism\n    Args:\n         d_model: dimensional input/output of the model\n         num_heads: number of attention heads\n    \"\"\"\n    def __init__(self, d_model:int, num_heads:int, max_seq_len:int):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.max_seq_len = max_seq_len\n        self.head_dim = d_model//num_heads\n        self.Wq = nn.Linear(d_model, d_model)\n        self.Wk = nn.Linear(d_model, d_model)\n        self.Wv = nn.Linear(d_model, d_model)\n        self.Er = nn.Parameter(torch.randn(max_seq_len*2-1, self.head_dim))\n        self.Wo = nn.Linear(d_model, d_model) # Turn the context vector to desired output dimension\n\n        self.logger = logging.getLogger(__name__)\n        \n    def forward(self, q:torch.Tensor, k:torch.Tensor, v:torch.Tensor,\n               mask:Optional[torch.Tensor]=None)->torch.Tensor:\n        \"\"\"\n        Forward pass of the relative attention mechanism.\n\n        Args:\n            q: Query tensor of shape (B, T, d_model).\n            k: Key tensor of shape (B, T, d_model).\n            v: Value tensor of shape (B, T, d_model).\n            mask: Attention mask of shape (1, 1, T, T) or None.\n            B: Batch size\n            T: Sequence Length\n            H: Number of heads\n            _: Placeholder\n\n        Returns:\n            Context vector of shape (B, T, d_model).\n        \"\"\"\n        B, T, _ = q.shape\n        H = self.num_heads\n        q = self.Wq(q).view(B,T,H, self.head_dim).transpose(1,2) #(B,H,T, d_k)\n        k = self.Wk(k).view(B,T,H, self.head_dim).transpose(1,2) #(B,H,T, d_k)\n        v = self.Wv(v).view(B,T,H, self.head_dim).transpose(1,2)\n\n        # Relative position attention\n        QEr = torch.matmul(q, self.Er.transpose(0,1)) # (B,H,T,2T-1)\n        scores = torch.matmul(q,k.transpose(2,3))     # (B,H,T,2T-1)\n        scores_relative = self._relative_shift(QEr)\n        scores = scores+scores_relative\n\n        if mask is not None:\n            scores = scores.masked_fill(mask[:,:,:T,:T]==0, float('-inf'))\n        attention = F.softmax(scores/(self.head_dim**.5), dim=-1)\n        context = torch.matmul(attention, v).transpose(1, 2).contiguous().view(B,T, self.d_model) # (B,T, d_model)\n        self.logger.debug(f\"Attention weights: {attention}\")\n        return self.Wo(context)\n    def _relative_shift(self, x:torch.Tensor)->torch.Tensor:\n        \"\"\"\n        Performs relative shifting for relative attention.\n\n        Args:\n            x: Input tensor.\n        Returns:\n            Shifted tensor.\n        \"\"\"\n        batch_size, num_heads, seq_length, _ = x.shape\n        x_padded = F.pad(x, (0,0,0,1))\n        x_reshaped = x_padded.view(batch_size, num_heads, seq_length+1, seq_length)\n        return x_reshaped[:,:,1:,:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:40:22.838364Z","iopub.execute_input":"2025-01-28T08:40:22.838596Z","iopub.status.idle":"2025-01-28T08:40:22.853856Z","shell.execute_reply.started":"2025-01-28T08:40:22.838577Z","shell.execute_reply":"2025-01-28T08:40:22.852990Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### 2. Encoder - Decoder Layer","metadata":{}},{"cell_type":"code","source":"class MusicTransformerEncoderLayer(nn.Module):\n    \"\"\"\n   Music Transformer encoder layer.\n\n   Args:\n       d_model: The input/output dimension of the model.\n       num_heads: The number of attention heads.\n       dff: The dimension of the feed-forward network.\n       dropout_rate: The dropout rate.\n   \"\"\"\n    def __init__(self, d_model:int, num_heads:int,\n                dff:int, dropout_rate:float,\n                max_seq_len:int):\n        super().__init__()\n        self.self_attn = RelativeAttention(d_model, num_heads, max_seq_len)\n        self.ffn = nn.Sequential(nn.Linear(d_model, dff),\n                                nn.ReLU(), nn.Linear(dff, d_model)) # Feed-forward upwards projection\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout_rate)\n    def forward(self, x:torch.Tensor, mask:torch.Tensor)->torch.Tensor:\n        \"\"\"\n       Forward pass of the encoder layer.\n\n       Args:\n           x: Input tensor of shape (B, T, d_model).\n           mask: Attention mask of shape (1, 1, T, T).\n\n       Returns:\n           Output tensor of shape (B, T, d_model).\n       \"\"\"\n        attn_output = self.self_attn(x, x, x, mask)\n        attn_output = self.dropout(attn_output)\n        x = self.norm1(x+attn_output)\n        ffn_output = self.ffn(x)\n        ffn_output = self.dropout(ffn_output)\n        x = self.norm2(x + ffn_output)\n        return x\n\nclass MusicTransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model:int, num_heads:int, dff:int,\n                dropout_rate:float, max_seq_len:int):\n        super().__init__()\n        self.self_attn = RelativeAttention(d_model, num_heads, max_seq_len)\n        self.cross_attn = RelativeAttention(d_model, num_heads, max_seq_len)\n        self.ffn = nn.Sequential(nn.Linear(d_model, dff),\n                                nn.ReLU(), nn.Linear(dff, d_model))\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout_rate)\n    def forward(self, x, enc_output, tgt_mask, memory_mask):\n        attn_output = self.dropout(self.self_attn(x, x, x, tgt_mask))\n        x = self.norm1(x + attn_output)\n        cross_attn_output = self.dropout(self.cross_attn(x, enc_output, enc_output,\n                                                        memory_mask))\n        x = self.norm2(x + cross_attn_output)\n        ffn_output = self.dropout(self.ffn(x))\n        return self.norm3(x + ffn_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:40:22.854989Z","iopub.execute_input":"2025-01-28T08:40:22.855208Z","iopub.status.idle":"2025-01-28T08:40:22.871185Z","shell.execute_reply.started":"2025-01-28T08:40:22.855190Z","shell.execute_reply":"2025-01-28T08:40:22.870586Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### 2.1 Music Transformer Model","metadata":{}},{"cell_type":"code","source":"class MusicTransformer(nn.Module):\n    \"\"\"\n    Music Transformer model.\n\n    Args:\n        num_classes: The number of classes (vocabulary size).\n        d_model: The input/output dimension of the model.\n        num_layers: The number of encoder layers.\n        num_heads: The number of attention heads.\n        dff: The dimension of the feed-forward network.\n        dropout_rate: The dropout rate.\n        max_seq_len: The maximum sequence length.\n    \"\"\"\n    def __init__(self, num_classes:int, d_model:int,\n                num_layers:int, num_heads:int, dff:int, \n                dropout_rate:float, max_seq_len:int, pad_token:int):\n        super().__init__()\n        self.embedding = nn.Embedding(num_classes, d_model)\n        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n        self.encoder = nn.ModuleList([MusicTransformerEncoderLayer(d_model, num_heads,\n                                                                  dff, dropout_rate, \n                                                                  max_seq_len)\n                                     for _ in range(num_layers)])\n        self.decoder = nn.ModuleList([MusicTransformerDecoderLayer(d_model, num_heads,\n                                                                  dff, dropout_rate,\n                                                                  max_seq_len)\n                                     for _ in range(num_layers)])\n        self.fc = nn.Linear(d_model, num_classes)\n        self.max_seq_len = max_seq_len\n        self.pad_token = pad_token\n\n        self._init_weights() \n\n    def forward(self, src:torch.Tensor, tgt:torch.Tensor)->torch.Tensor:\n        \"\"\"\n        Forward pass of the Music Transformer.\n\n        Args:\n            x: Input tensor of shape (B, T).\n\n        Returns:\n            Output tensor of shape (B, T, num_classes).\n        \"\"\"\n\n        # Embedding with positional encoding\n        # 1.Encoder Processing\n        B, T_src = src.shape\n\n        src_pos = torch.arange(T_src, device = src.device).unsqueeze(0).expand(B,T_src)\n        src_emb = self.embedding(src) + self.pos_embedding(src_pos)\n\n        # 2.Decoder Processing\n        B, T_tgt = tgt.shape\n        \n        tgt_pos = torch.arange(T_tgt, device = tgt.device).unsqueeze(0).expand(B,T_tgt)\n        tgt_emb = self.embedding(tgt) + self.pos_embedding(tgt_pos)\n\n        # Masks\n        src_padding_mask = (src_emb[:,:,0] != self.pad_token).unsqueeze(1).unsqueeze(2)\n\n        tgt_padding_mask = (tgt_emb[:,:,0] != self.pad_token).unsqueeze(1).unsqueeze(2)\n        tgt_casual_mask = torch.tril(torch.ones((T_tgt, T_tgt),\n                                               device = tgt.device)).unsqueeze(0).unsqueeze(0)\n        tgt_mask = tgt_padding_mask + tgt_casual_mask\n\n        # Encode\n        encoder_output = src_emb\n        for layer in self.encoder:\n            encoder_output = layer(encoder_output, src_padding_mask)\n        # Decode\n        decoder_output = tgt_emb\n        for layer in self.decoder:\n            decoder_output = layer(decoder_output, encoder_output, tgt_mask, src_padding_mask)\n        return self.fc(tgt)\n    \n    def _init_weights(self):\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    nn.init.zero_(module.bias)\n            elif isinstance(module, nn.LayerNorm):\n                nn.init.ones_(module.weight)\n                nn.init.zeros_(module.bias)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:40:22.872062Z","iopub.execute_input":"2025-01-28T08:40:22.872352Z","iopub.status.idle":"2025-01-28T08:40:22.888728Z","shell.execute_reply.started":"2025-01-28T08:40:22.872323Z","shell.execute_reply":"2025-01-28T08:40:22.887979Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class LabelSmoothingLoss(nn.Module):\n    \"\"\"\n    Cross-entropy loss with label smoothing.\n\n    Args:\n        num_classes: Number of output classes.\n        smoothing: Smoothing factor (alpha). Default is 0.1.\n        ignore_index: Index to ignore in the target.\n    \"\"\"\n    def __init__(self, num_classes:int, smoothing:float=0.1,\n                ignore_index:int=1):\n        super().__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.ignore_index = ignore_index\n    def forward(self, pred, target):\n        \"\"\"\n        Compute the label-smoothing loss.\n\n        Args:\n            pred: Predictions of shape (B, T, num_classes).\n            target: Ground truth of shape (B, T).\n\n        Returns:\n            Smoothed cross-entropy loss.\n        \"\"\"\n        pred = pred.log_softmax(dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing/(self.num_classes-1))\n            true_dist.scatter_(2, target.unsqueeze(-1), 1.0-self.smoothing)\n            if self.ignore_index>0:\n                true_dist.masked_fill_((target==self.ignore_index).unsqueeze(-1),0)\n        return torch.mean(torch.sum(-true_dist*pred, dim=-1)) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:40:22.889616Z","iopub.execute_input":"2025-01-28T08:40:22.889829Z","iopub.status.idle":"2025-01-28T08:40:22.906566Z","shell.execute_reply.started":"2025-01-28T08:40:22.889811Z","shell.execute_reply":"2025-01-28T08:40:22.905787Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## **Utility Functions**","metadata":{}},{"cell_type":"code","source":"def build_music_transformer(num_classes:int, config:dict):\n    \"\"\"\n    Build a MusicTransformer model using the provided configuration.\n\n    Args:\n        num_classes (int): The number of output classes (vocabulary size).\n        config (dict): A dictionary containing model hyperparameters.\n\n    Returns:\n        MusicTransformer: A configured MusicTransformer instance.\n    \"\"\"\n    return MusicTransformer(num_classes = num_classes,\n                           d_model = config[\"d_model\"],\n                           num_layers = config[\"num_layers\"],\n                           num_heads = config[\"num_heads\"],\n                           dff = config[\"dff\"],\n                           dropout_rate = config[\"dropout_rate\"],\n                           max_seq_len = config[\"max_seq_len\"],\n                           pad_token = config[\"pad_token\"])\n\ndef get_loss_function(loss_type:str, num_classes:int,\n                      smoothing:float, pad_token:int):\n    \"\"\"\n    Get the appropriate loss function based on the provided type\n\n    Args:\n        loss_type (str): The type of loss function to use (\"cross_entropy\" or custom).\n        num_classes (int): The number of output classes.\n        smoothing (float): Label smoothing value for cross-entropy.\n        pad_token (int): Padding token ID.\n\n    Returns:\n        nn.Module: The loss function.\n    \"\"\"\n    if loss_type == \"cross_entropy\":\n        return LabelSmoothingLoss(num_classes, smoothing, pad_token)\n    elif loss_type == \"mse\":\n        return nn.MSELoss()\n    else:\n        raise ValueError(f\"Unsupported loss type: {loss_type}\")\n\ndef get_optimizer(optimizer_type:str,\n                  model:nn.Module,\n                  learning_rate:float):\n    \"\"\"\n    Get the optimizer based on the provided type.\n\n    Args:\n        optimizer_type (str): The type of optimizer (\"adam\", \"adamw\", etc.).\n        model (nn.Module): The model to optimize.\n        learning_rate (float): The learning rate.\n\n    Returns:\n        Optimizer: A configured optimizer instance.\"\"\"\n    if optimizer_type == \"adam\":\n        return optim.Adam(model.parameters(), lr=learning_rate)\n    elif optimizer_type == \"adamw\":\n        return optim.AdamW(model.parameters(), lr=learning_rate)\n    else:\n        raise ValueError(f\"Unsupported optimizer: {optimizer_type}\")\n\ndef get_scheduler(scheduler_type:str,\n                  optimizer, **kwargs):\n    \"\"\"\n    Get the learning rate scheduler.\n\n    Args:\n        scheduler_type (str): Type of scheduler (\"cosine\", \"step\", etc.).\n        optimizer (Optimizer): The optimizer instance.\n        **kwargs: Additional parameters for specific schedulers.\n\n    Returns:\n        Scheduler: A configured scheduler instance.\n    \"\"\"\n    if scheduler_type == \"cosine\":\n        return optim.lr_scheduler.CosineAnnealingLR(optimizer,\n                                                    T_max=kwargs.get(\"T_max\",\n                                                                     10))\n    elif scheduler_type == \"step\":\n        return optim.lr_scheduler.StepLR(optimizer,\n                                         step_size=kwargs.get(\"step_size\",10))\n    else:\n        raise ValueError(f\"Unsupported scheduler: {scheduler_type}\")\n\ndef evaluate_model(model:nn.Module,\n                   dataloader:DataLoader,\n                   criterion:nn.Module,\n                   metrics:dict,\n                   device:torch.device):\n    \"\"\"\n    Evaluate the model on a dataset.\n\n    Args:\n        model (nn.Module): The model to evaluate.\n        dataloader (DataLoader): The dataloader for evaluation.\n        criterion (nn.Module): The loss function.\n        metrics (dict): A dictionary of metric functions.\n\n    Returns:\n        tuple: Average loss and a dictionary of average metric values.\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    total_metrics = {name:0 for name in metrics.keys()}\n\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch[\"input_ids\"].to(device)\n            targets = batch[\"labels\"].to(device)\n\n            decoder_inputs = input_ids[:,:-1]\n            decoder_targets = targets[:,1:]\n\n            outputs = model(decoder_inputs)\n            loss = criterion(outputs.view(-1, outputs.size(-1)),\n                             decoder_targets.contiguous().view(-1)).item()\n            total_loss += loss.item()\n\n            for name, metric_fn in metrics.items():\n                total_metrics[name] += metric_fn(outputs, targets)\n    avg_loss = total_loss/len(dataloader)\n    avg_metrics = {name:value/len(dataloader) for name,value in total_metrics.items()}\n    return avg_loss, avg_metrics\n\ndef freeze_layers(model:nn.Module, freeze_embedding=True,\n                  freeze_layers=[]):\n    \"\"\"\n    Freeze specified layers of the model.\n\n    Args:\n        model (nn.Module): The model to modify.\n        freeze_embedding (bool): Whether to freeze the embedding layer.\n        freeze_layers (list[int]): List of layer indices to freeze.\n    \"\"\"\n    if freeze_embedding:\n        for param in model.embedding.parameters():\n            param.requires_grad = False\n    for idx in freeze_layers:\n        for param in model.layers[idx].parameters():\n            param.requires_grad = False\n\n\ndef accuracy_fn(predictions, targets, pad_token):\n    \"\"\"Compute accuracy of predictions\"\"\"\n    _, pred_ids = torch.max(predictions, dim=-1)\n    correct = (pred_ids==targets).float()\n    mask = targets != pad_token # Ignore padding tokens\n    return (correct*mask).sum().item()/mask.sum().item()\n\ndef save_generated_sequence(sequence, output_path):\n    midi = MidiFile()\n    track = Instrument(program=0)\n    for pitch in sequence:\n        track.notes.append(Note(velocity=64, pitch = pitch,\n                               start = 0, end=480))\n    midi.instruments.append(track)\n    midi.dump(output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:40:22.907348Z","iopub.execute_input":"2025-01-28T08:40:22.907628Z","iopub.status.idle":"2025-01-28T08:40:22.921447Z","shell.execute_reply.started":"2025-01-28T08:40:22.907608Z","shell.execute_reply":"2025-01-28T08:40:22.920724Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"token_paths = list(Path('/kaggle/working/preprocessedv2/').glob(\"**/*.json\"))\n\ntokenizer_config = TokenizerConfig(num_velocities=32, use_chords=True,\n                                   use_rests=True, use_tempos=True,\n                                   use_time_signatures=True,\n                                   beat_res={(0, 4): 8, (4, 12): 4})\n\ndataset = MaestroDataset(file_paths = token_paths,\n                        max_seq=512, min_seq=384,\n                        pad_token=0, preprocess=True,\n                        tokenizer_config = tokenizer_config,\n                        output_dir=Path(\"/kaggle/working/\"))\nsubset_train, subset_valid = create_subsets(dataset, split_ratio = [0.3])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:44:02.339506Z","iopub.execute_input":"2025-01-28T08:44:02.339797Z","iopub.status.idle":"2025-01-28T08:44:02.373951Z","shell.execute_reply.started":"2025-01-28T08:44:02.339775Z","shell.execute_reply":"2025-01-28T08:44:02.373235Z"}},"outputs":[{"name":"stderr","text":"Preprocessing MIDI files: 0it [00:00, ?it/s]\nLoading tokenized files: 0it [00:00, ?it/s]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## **Training Loop**","metadata":{}},{"cell_type":"code","source":"from torch.cuda.amp import GradScaler, autocast\n\nscaler = GradScaler()\naccumulation_steps = 4 \n\nwith open(\"/kaggle/input/jsonconfigfile/config.json\", \"r\") as f:\n    config = json.load(f)\n    model_params = config[\"model_params\"]\n    training_params = config[\"training_params\"]\n    scheduler_params = config[\"scheduler\"]\n    data_paths = config[\"data_paths\"]\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model_ = MusicTransformer(num_classes = model_params[\"num_classes\"],\n                            d_model = model_params[\"d_model\"],\n                            num_layers = model_params[\"num_layers\"],\n                            num_heads = model_params[\"num_heads\"],\n                            dff = model_params[\"dff\"],\n                            dropout_rate = model_params[\"dropout_rate\"],\n                            max_seq_len = model_params[\"max_seq_len\"],\n                            pad_token = model_params[\"pad_token\"]).to(device)\n\n    criterion = get_loss_function(config[\"loss_function\"], model_.embedding.num_embeddings,\n                                  smoothing=training_params[\"smoothing\"],\n                                  pad_token = model_params[\"pad_token\"])\n    optimizer = get_optimizer(config[\"optimizer\"], model_,\n                              learning_rate=training_params[\"learning_rate\"])\n\n    # Learning rate scheduler\n    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n    scheduler = get_scheduler(scheduler_params[\"type\"], optimizer,\n                              step_size = scheduler_params[\"step_size\"] ,\n                              gamma = scheduler_params[\"gamma\"])\n    # Ensure batch_size is not larger than the dataset\n    batch_size = min(training_params[\"batch_size\"], len(subset_train))\n    # Dataloader\n    train_loader = DataLoader(subset_train, batch_size=batch_size,\n                              shuffle=True, collate_fn=lambda x:collate_fn(x, model_params[\"pad_token\"]))\n    val_loader   = DataLoader(subset_valid, batch_size=batch_size,\n                              shuffle=False, collate_fn=lambda x:collate_fn(x, model_params[\"pad_token\"]))\n\n    # Metrics\n    metrics = {\"accuracy\":accuracy_fn}\n\n    # Training Loop\n    best_val_loss = float(\"inf\")\n    os.makedirs(data_paths[\"checkpoint_dir\"], exist_ok=True)\n\n    for epoch in range(training_params[\"num_epochs\"]):\n        model_.train()\n        epoch_loss = 0\n        for i, inputs in enumerate(tqdm(train_loader\n                                        , desc=f\"Epoch {epoch+1}/{training_params[\"num_epochs\"]}\")):\n            inputs_ids = inputs[\"input_ids\"].to(device)\n            targets    = inputs[\"labels\"].to(device)\n\n            # Shift targets for autoregressive modeling\n            encoder_inputs = inputs_ids[:,:-1]\n            decoder_inputs = inputs_ids[:,:-1]\n            decoder_targets = targets[:,1:]\n\n            \n            # Forward Pass\n            with autocast():\n                outputs = model_(encoder_inputs, decoder_inputs)\n                loss = criterion(outputs.view(-1, outputs.size(-1)),\n                                 decoder_targets.contiguous().view(-1))/accumulation_steps\n\n            # Backward pass and optimization\n            scaler.scale(loss).backward()\n            if (i+1)%accumulation_steps==0:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                torch.nn.utils.clip_grad_norm_(model_.parameters(),\n                                           max_norm=training_params[\"clip_value\"])\n            epoch_loss += loss.item()*accumulation_steps\n\n        train_loss = epoch_loss/len(train_loader)\n        print(f\"Epoch {epoch+1} | Training Loss : {train_loss:.4f}\")\n\n        # Validation Loss\n        val_loss, val_metrics = evaluate_model(model = model_,\n                                               dataloader = val_loader,\n                                               criterion=criterion,\n                                               metrics = metrics,\n                                               device = device)\n        if val_loss<best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model_.state_dict(), os.path.join(data_paths[\"checkpoint_dir\"],\n                                                        \"best_model.pth\"))\n        print(f\"Epoch {epoch + 1} | Validating Loss: {val_loss:.4f} | Validating Metrics: {val_metrics}\")\n\n        # Generate and save music samples\n        if epoch%1==0: # Generate for every epoch\n            model_.eval()\n            with torch.no_grad():\n                start_sequence = torch.tensor([random.randint(1, model_params[\"num_classes\"]-1)],\n                                              device=device).unsqueeze(0) # Random start token\n                generated_sequence = start_sequence.clone()\n                for _ in range(training_params[\"generation_length\"]):\n                    output = model_(generated_sequence)\n                    next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)\n                    generated_sequence = torch.cat([generated_sequence, next_token], dim=1)\n                    if next_token.item() == model_params[\"pad_token\"]: # Stop on pad token\n                        break\n                print(f\"Generated Sequence : {generated_sequence.tolist()}\")\n                save_generated_sequence(generated_sequence.squeeze().tolist(),\n                                       f\"generated_epoch_{epoch+1}.mid\")\n\n        # Save checkpoint\n        torch.save(model_.state_dict(),os.path.join(data_paths[\"checkpoint_dir\"],\n                                                    f\"model_epoch{epoch + 1}.pth\"))\n        scheduler.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:40:23.019854Z","iopub.status.idle":"2025-01-28T08:40:23.020093Z","shell.execute_reply":"2025-01-28T08:40:23.019997Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Testing purpose with a single sample**","metadata":{}},{"cell_type":"code","source":"try:\n    import torch\n    from pathlib import Path\n    from typing import List, Dict, Any\n    from mido import MidiFile, MidiTrack, Message\n    from miditok import REMI, TokenizerConfig\n    \n    with open(\"/kaggle/input/jsonconfigfile/config.json\", \"r\") as f:\n        config = json.load(f)\n        model_params = config[\"model_params\"]\n        training_params = config[\"training_params\"]\n        scheduler_params = config[\"scheduler\"]\n        data_paths = config[\"data_paths\"]\n        \n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        midi = MidiFile()\n        track = MidiTrack()\n        track.append(Message('program_change', program=0, time=0))  # Set instrument to piano\n        track.append(Message('note_on', note=60, velocity=64, time=0)) # C4\n        track.append(Message('note_off', note=60, velocity=64, time=480))\n        track.append(Message('note_on', note=64, velocity=64, time=480)) # E4\n        track.append(Message('note_off', note=64, velocity=64, time=480))\n        midi.tracks.append(track)\n        midi_path = Path(\"test_midi.mid\")\n        midi.save(str(midi_path))\n        \n        \n        \n        dataset.data = [midi_path]\n        sample = dataset[0]\n        \n        model = MusicTransformer(num_classes = model_params[\"num_classes\"],\n                            d_model = model_params[\"d_model\"],\n                            num_layers = model_params[\"num_layers\"],\n                            num_heads = model_params[\"num_heads\"],\n                            dff = model_params[\"dff\"],\n                            dropout_rate = model_params[\"dropout_rate\"],\n                            max_seq_len = model_params[\"max_seq_len\"],\n                            pad_token = model_params[\"pad_token\"]).to(device)\n        \n        criterion = get_loss_function(config[\"loss_function\"], model_.embedding.num_embeddings,\n                                  smoothing=training_params[\"smoothing\"],\n                                  pad_token = model_params[\"pad_token\"])\n        optimizer = get_optimizer(config[\"optimizer\"], model_,\n                              learning_rate=training_params[\"learning_rate\"])\n        \n        # Learning rate scheduler\n        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n        scheduler = get_scheduler(scheduler_params[\"type\"], optimizer,\n                              step_size = scheduler_params[\"step_size\"] ,\n                              gamma = scheduler_params[\"gamma\"])\n        \n        # Training Loop (Single Sample Test)\n        model_.train()\n        input_ids = sample[\"input_ids\"].unsqueeze(0)\n        targets = sample[\"labels\"].unsqueeze(0)\n        decoder_inputs = input_ids[:, :-1]\n        decoder_targets = targets[:, 1:]\n        outputs = model_(decoder_inputs)\n        loss = criterion(outputs.view(-1, outputs.size(-1)), decoder_targets.contiguous().view(-1))\n        \n        print(\"Output Shape:\", outputs.shape)\n        print(\"Loss:\", loss.item())\n        \n        # Clean up the created midi file\n        os.remove(midi_path)\nexcept Exception as e:\n    logger.error(f\"Error during single sample test: {e}\")\nfinally:\n    if os.path.exists(midi_path):\n        os.remove(midi_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:45:47.519570Z","iopub.execute_input":"2025-01-28T08:45:47.519866Z","iopub.status.idle":"2025-01-28T08:45:47.532220Z","shell.execute_reply.started":"2025-01-28T08:45:47.519844Z","shell.execute_reply":"2025-01-28T08:45:47.531279Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}