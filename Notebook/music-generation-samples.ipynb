{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1246998,"sourceType":"datasetVersion","datasetId":716027}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install miditoolkit\n!pip install torchtoolkit\n\n\nimport json\nimport torch\nimport logging\nimport pandas as pd\nfrom pretty_midi import PrettyMIDI\n\nfrom pathlib import Path\nfrom typing import Callable, Any, Dict, List, Optional\nfrom miditoolkit import MidiFile\nfrom miditok import REMI, TokenizerConfig\nfrom torchtoolkit.data import create_subsets\nfrom tqdm import tqdm\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch import LongTensor\nfrom torch.nn.utils.rnn import pad_sequence\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AbstractMusicDataset(Dataset):\n    \"\"\"\n    Abstract base class for music dataset\n\n    Args:\n        data_path (List[Path]) : Path to the dataset\n        preprocess_fn (Callable): Function to preprocess a single sample\n        max_seq_len (int) : Maximum sequence length of the data\n        pad_token (int) : token used for padding sequences\n    \"\"\"\n    def __init__(self, data_path:List[Path],\n                preprocess_fn:Callable, max_seq_len:int,\n                pad_token:int, **kwargs):\n        self.data_path = data_path\n        self.preprocess_fn = preprocess_fn\n        self.max_seq_len = max_seq_len\n        self.pad_token = pad_token\n        self.kwargs = kwargs\n        self.data = self.load_data(**kwargs)\n    def load_data(self, **kwargs)->List[Any]:\n        \"\"\"Load any data from specified data path, Override this in subclasses\"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method\")\n\n    def preprocess(self, sample:Any)->Dict[str,Any]:\n        \"\"\"Preprocess a single data sample\"\"\"\n        return self.preprocess_fn(sample, max_seq_len=self.max_seq_len,\n                                 pad_token=self.pad_token)\n    def __len__(self)->int:\n        return len(self.data)\n    def __getitem__(self, idx:int)->Dict[str,Any]:\n        return self.preprocess(self.data[idx])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MIDIDataset(AbstractMusicDataset):\n    def __init__(self, data_path:List[Path],preprocess_fn:Callable,\n                 max_seq_len:int, pad_token:int):\n        super().__init__(data_path, preprocess_fn,\n                         max_seq_len, pad_token)\n    def load_data(self)->List[MidiFile]:\n        \"\"\"Load MIDI file from dataset directory\"\"\"\n        try:\n            files = list(self.data_path.glob(\"*.midi\"))\n            if not files:\n                logger.warning(f\"No MIDI files found in: {self.data_path}\")\n                return []\n            return [MidiFile(str(file)) for file in files]\n        except FileNotFoundError:\n            logger.error(f\"MIDI directory not found: {self.data_path}\")\n            return []\n        except Exception as e:\n            logger.error(f\"Error loading MIDI files: {e}\")\n            return []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CSVDataset(AbstractMusicalDataset):\n    def __init__(self, data_path:List[Path],preprocess_fn:Callable,\n                 csv_filename:str, max_seq_len:int):\n        super().__init__(data_path, preprocess_fn, max_seq_len, pad_token,\n                        csv_filename = csv_filename)\n        self.csv_filename = csv_filename\n    def load_data(self, csv_filename:str)->List[Dict[str, Any]]:\n        \"\"\"Load the CSV file into a list of dictionaries\"\"\"\n        csv_path = self.data_path/self.csv_filename # Use Path object for joining\n        try:\n            df = pd.read_csv(str(csv_path)) \n            return df.to_dict(orient=\"records\")\n        except FileNotFoundError:\n            logger.error(f\"CSV file not found: {csv_path}\")\n            return []\n        except pd.errors.ParseError as e:\n            logger.error(f\"Error parsing CSV file: {e}\")\n            return []\n        except Exception as e:\n            logger.error(f\"An unexpected error occured while loading or parsing CSV file: {e}\")\n            return []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class JSONDataset(AbstractMusicalDataset):\n    def __init__(self, data_path:List[Path], preprocess_fn:Callable,\n                max_seq_len:int, pad_token:int, json_filename:str):\n        super().__init__(data_path, preprocess_fn, max_seq_len, pad_token,\n                        json_filename = json_filename)\n        self.json_filename = json_filename\n    def load_data(self, json_filename:str)->List[Dict[str, Any]]:\n        \"\"\"Load a JSON file containing the dataset.\"\"\"\n        json_path = self.data_path[0]/self.json_filename\n        try:\n            with open(str(json_path), \"r\") as f:\n                return json.load(f)\n        except FileNotFoundError:\n            logger.error(f\"JSON file not found: {json_path}\")\n            return []\n        except json.JSONDecodeError as e:\n            logger.error(f\"Error decoding JSON: {e}\")\n            return []\n        except Exception as e:\n            logger.error(f\"An unexpected error occured while loading or parsing JSON file: {e}\")\n            return []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def midi_preprocess(sample:PrettyMIDI, max_seq_len:int, pad_token:int,\n                   sos_token:int, eos_token:int\n                    , default_sample=None)->Dict[str, torch.Tensor]:\n    \"\"\"\n    Preprocess a MIDI sample: tokenize, truncate, and pad.\n\n    Args:\n        sample (MidiFile): MIDI file sample.\n        max_seq_len (int): Maximum sequence length.\n        pad_token (int): Padding token.\n    Returns:\n        Dict[str, torch.Tensor]: Preprocessed sample with input IDs and labels.\n    \"\"\"\n    try:\n        if not sample.instruments:\n            logger.warning(\"MIDI file has no instruments.\")\n            return default_sample or {\n                \"input_ids\":torch.full((max_seq_len), pad_token, dtype=torch.long)\n            }\n        tokens = []\n        for instrument in sample.instruments:\n            for note in instrument.notes:\n                tokens.append(note.pitch)\n        tokens = [sos_token] + tokens[:max_seq_len-2]+[eos_token]\n        padded_tokens = torch.nn.functional.pad(torch.tensor(tokens,\n                                                             dtype = torch.long),\n                                               (0, max_seq_len-len(tokens)),\n                                               value=pad_token)\n        logger.info(f\"Processed MIDI file : {len(tokens)} tokens\")\n        return {\"input_ids\":padded_tokens,\n               \"labels\":padded_tokens}\n    except Exception as e:\n        logger.error(f\"Error preprocessing MIDI: {e}\")\n        return default_sample or {\n            \"input_ids\":torch.full((max_seq_len), pad_token, dtype=torch.long),\n            \"labels\":torch.full(size(max_seq_len), pad_token, dtype=torch.long)}\n\ndef csv_preprocess(sample:Dict[str, Any],\n                  max_seq_len:int,\n                  pad_token:int,\n                  sos_token:int,\n                  eos_token:int,\n                  default_sample:Dict[str, torch.Tensor]=None)->Dict[str, torch.Tensor]:\n    \"\"\"\n    Preprocess CSV data for model input.\n\n    Args:\n        sample: A dictionary containing 'notes' as a key with a list of integer tokens.\n        max_seq_len: The maximum sequence length.\n        pad_token: The token used for padding.\n        sos_token: The start-of-sequence token.\n        eos_token: The end-of-sequence token.\n        default_sample: Optional default sample for invalid data.\n\n    Returns:\n        A dictionary with 'input_ids' and 'labels' tensors.\n    \"\"\"\n    try:\n        # For validating input format\n        if not isinstance(sample, dict) or \"notes\" not in sample:\n            raise ValueError(\"Invalid CSV sample format: missing 'notes' key.\")\n        tokens = sample[\"notes\"]\n        # Check token validity\n        if not all(isinstance(token, int) for token in tokens):\n            raise TypeError(\"CSV tokens must be integers.\")\n\n        # Add special tokens and truncats\n        tokens = [sos_token] + tokens[:max_seq_len-2]+[eos_token]\n        # Pad the sequence\n        padded_tokens = torch.nn.functional.pad(torch.tensor(tokens, dtype=torch.long),\n                                               (0, max_seq_len-len(tokens)),\n                                               value=pad_token)\n        logger.info(f\"Processed CSV data: {len(tokens)} tokens (max {max_seq_len}).\")\n        return {\n            \"input_ids\": padded_tokens,\n            \"labels\":padded_tokens\n        }\n    except (ValueError, TypeError) as e:\n        logger.error(f\"Error preprocessing CSV: {e}\")\n        return default_sample or {\n            \"input_ids\":torch.full((max_seq_len), pad_token, dtype=torch.long),\n            \"labels\":torch.full((max_seq_len), pad_token, dtype=torch.long)\n        }\n\ndef json_preprocess(sample:Dict[str, Any],\n                   max_seq_len:int,\n                   pad_token:int,\n                   sos_token:int,\n                   default_sample:Dict[str, torch.Tensor]=None)->Dict[str,torch.Tensor]:\n    \"\"\"\n    Preprocess JSON data for model input.\n\n    Args:\n        sample: A dictionary containing 'sequence' as a key with a list of integer tokens.\n        max_seq_len: The maximum sequence length.\n        pad_token: The token used for padding.\n        sos_token: The start-of-sequence token.\n        eos_token: The end-of-sequence token.\n        default_sample: Optional default sample for invalid data.\n\n    Returns:\n        A dictionary with 'input_ids' and 'labels' tensors.\n    \"\"\"\n    try:\n        # Validate input format\n        if not isinstance(sample, dict) or \"sequence\" not in sample:\n            raise ValueError(\"Invalid JSON sample format: missing 'sequence' key.\")\n        tokens = sample[\"sequence\"]\n        \n        # Check token validity\n        if not all(isinstance(token,int) for token in tokens):\n            raise TypeError(\"JSON tokens must be integers.\")\n        # Add special tokens and truncate\n        tokens = [sos_token]+tokens[:max_seq_len-2]+[eos_token]\n        # Pad the sequence\n        padded_tokens = torch.nn.functional.pad(torch.tensor(tokens, dtype=torch.long),\n                                               (0, max_seq_len-len(tokens)), value=pad_token)\n        logger.info(f\"Processed JSON data: {len(tokens)} tokens (max {max_seq_len}).\")\n        return {\"input_ids\":padded_tokens,\n               \"labels\":padded_tokens}\n    except (ValueError, TypeError) as e:\n        logger.error(f\"Error preprocessing JSON:{e}\")\n        return default_sample or {\"input_ids\":torch.full((max_seq_len),pad_token, dtype=torch.long),\n                                 \"labels\":torch.full((max_seq_len), pad_token, dtype=torch.long)}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Preprocessing for the specific MaestroV2 Dataset**","metadata":{}},{"cell_type":"code","source":"class MaestroDataset(Dataset):\n    \"\"\"\n    A dataset for processing Maestro MIDI files.\n\n    Args:\n        file_paths (list): List of paths to MIDI or JSON files.\n        min_seq (int): Minimum sequence length.\n        max_seq (int): Maximum sequence length.\n        tokenizer_config (TokenizerConfig, optional): Configuration for the tokenizer (default: None)\n        pad_token (int): The token used for padding sequences.\n        preprocess (bool): Whether to preprocess and save tokenized files.\n        output_dir (Path): Directory to save preprocessed token files.\n\n    Attributes:\n        samples (list): List of tokenized sequences.\n    \"\"\"\n    def __init__(self, file_paths:List[Path],\n                min_seq:int,\n                max_seq:int,\n                pad_token:int,\n                tokenizer_config: TokenizerConfig=None,\n                preprocess:bool=True,\n                output_dir:Path=None):\n        self.samples = []\n        self.pad_token = pad_token\n        # Preprocessing if needed\n        if preprocess and output_dir is not None:\n            self._preprocessing_(file_paths, tokenizer_config, output_dir)\n            file_paths = List(output_dir.glob(\"*.json\"))\n        # Load preprocessed tokens\n        self.load_samples(file_paths, min_seq, max_seq)\n    def _preprocessing_(self, file_paths:List[Path],\n                       tokenizer_config:TokenizerConfig,\n                       output_dir:Path):\n        output_dir.mkdir(parents=True, exist_ok=True)\n        for i in tqdm(file_paths, desc=\"Preprocessing MIDI files\"):\n            try:\n                if i.suffix in [\"MIDI\", \"MID\", \"midi\", \"mid\"]:\n                    midi = MidiFile(i)\n                    tokenizer = REMI(tokenizer_config) if tokenizer_config is not None else REMI()\n                    all_tracks_tokens = [tokenizer.midi_to_tokens(midi)[0].ids \n                                         for track in midi.tracks if len(track)>0]\n                    tokens = [token for track in all_tracks_tokens for token in track]\n                else:\n                    continue # Skip non-MIDI files\n                # Save tokens to JSON\n                output_file = output_dir/f\"{i.stem}_tokens.json\"\n                with open(output_file, \"w\") as f:\n                    json.dump({\"ids\":tokens}, f)\n            except Exception as e:\n                logger.warning(f\"Error processing {i}: {e}\")\n    def load_samples(self, file_paths:List[Path],\n                        min_seq:int,\n                        max_seq:int):\n        \"\"\"Load tokenized samples and create sequences\"\"\"\n        for file_path in tqdm(file_paths, desc=\"Loading tokenized files\"):\n            try:\n                with open(file_path, \"r\") as f:\n                    tokens = json.load(f)[\"ids\"]\n                # Create fixed length sequences\n                i = 0\n                while i<len(tokens):\n                    if i>=len(tokens)-min_seq:\n                        break\n                    self.samples.append(LongTensor(tokens[i:i+max_seq]))\n                    i+=len(self.samples[-1])\n            except Exception as e:\n                logger.warning(f\"Error loading {file_path}: {e}\")\n    def __getitem__(self, idx)->Dict[str, LongTensor]:\n        return {\"input_ids\":self.samples[idx],\n               \"labels\":self.samples[idx]}\n    def __len__(self)->int:\n        return len(self.samples)\n    def __regr__(self):\n        return self.__str__()\n    def __str__(self)->str:\n        return \"No data loaded\" if len(self)==0 else f\"{len(self.samples)} samples\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch:List[Dict[str, LongTensor]],\n              pad_token:int)->Dict[str,LongTensor]:\n    \"\"\"\n    Collate function for dynamic padding.\n    Args:\n        batch: List of dictionaries, each containing 'input_ids' and 'labels'.\n        pad_token: Token used for padding.\n    Returns:\n        A dictionary with padded 'input_ids' and 'labels' as LongTensor.\n    \"\"\"\n    input_ids = [item[\"input_ids\"] for item in batch]\n    labels    = [item[\"labels\"] for item in batch]\n\n    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=pad_token).long()\n    labels = pad_sequence(labels, batch_first=True, padding_value=pad_token).long()\n\n    return {\"input_ids\":input_ids, \"labels\":labels}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Model Architecture**\n### 1. Attention Mechanism","metadata":{}},{"cell_type":"code","source":"class RelativeAttention(nn.Module):\n    \"\"\"\n    Relative self-attention mechanism\n    Args:\n         d_model: dimensional input/output of the model\n         num_heads: number of attention heads\n    \"\"\"\n    def __init__(self, d_model:int, num_heads:int, max_seq_len:int):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.max_seq_len = max_seq_len\n        self.head_dim = d_model//num_heads\n        self.Wq = nn.Linear(d_model, d_model)\n        self.Wk = nn.Linear(d_model, d_model)\n        self.Wv = nn.Linear(d_model, d_model)\n        self.Er = nn.Parameter(torch.randn(max_seq_len*2-1, self.head_dim))\n        self.Wo = nn.Linear(d_model, d_model) # Turn the context vector to desired output dimension\n\n        self.logger = logging.getLogger(__name__)\n        \n    def forward(self, q:torch.Tensor, k:torch.Tensor, v:torch.Tensor,\n               mask:Optional[torch.Tensor]=None)->torch.Tensor:\n        \"\"\"\n        Forward pass of the relative attention mechanism.\n\n        Args:\n            q: Query tensor of shape (B, T, d_model).\n            k: Key tensor of shape (B, T, d_model).\n            v: Value tensor of shape (B, T, d_model).\n            mask: Attention mask of shape (1, 1, T, T) or None.\n            B: Batch size\n            T: Sequence Length\n            H: Number of heads\n            _: Placeholder\n\n        Returns:\n            Context vector of shape (B, T, d_model).\n        \"\"\"\n        B, T, _ = q.shape\n        H = self.num_heads\n        q = self.Wq(q).view(B,T,H, self.head_dim).transpose(1,2) #(B,H,T, d_k)\n        k = self.Wk(k).view(B,T,H, self.head_dim).transpose(1,2) #(B,H,T, d_k)\n        v = self.Wv(v).view(B,T,H, self.head_dim).transpose(1,2)\n\n        # Relative position attention\n        QEr = torch.matmul(q, self.Er.transpose(0,1)) # (B,H,T,2T-1)\n        scores = torch.matmul(q,k.transpose(2,3))     # (B,H,T,2T-1)\n        scores_relative = self._relative_shift(QEr)\n        scores = scores+scores_relative\n\n        if mask is not None:\n            scores = scores.masked_fill(mask[:,:,:T,:T]==0, float('-inf'))\n        attention = F.softmax(scores/(self.head_dim**.5), dim=-1)\n        context = torch.matmul(attention, v).transpose(1, 2).contiguous().view(B,T, self.d_model) # (B,T, d_model)\n        self.logger.debug(f\"Attention weights: {attention}\")\n        return self.Wo(context)\n    def _relative_shift(self, x:torch.Tensor)->torch.Tensor:\n        \"\"\"\n        Performs relative shifting for relative attention.\n\n        Args:\n            x: Input tensor.\n        Returns:\n            Shifted tensor.\n        \"\"\"\n        batch_size, num_heads, seq_length, _ = x.shape\n        x_padded = F.pad(x, (0,0,0,1))\n        x_reshaped = x_padded.view(batch_size, num_heads, seq_length+1, seq_length)\n        return x_reshaped[:,:,1:,:]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Encoder - Decoder Layer","metadata":{}},{"cell_type":"code","source":"class MusicTransformerEncoderLayer(nn.Module):\n    \"\"\"\n   Music Transformer encoder layer.\n\n   Args:\n       d_model: The input/output dimension of the model.\n       num_heads: The number of attention heads.\n       dff: The dimension of the feed-forward network.\n       dropout_rate: The dropout rate.\n   \"\"\"\n    def __init__(self, d_model:int, num_heads:int,\n                dff:int, dropout_rate:float,\n                max_seq_len:int):\n        super().__init__()\n        self.self_attn = RelativeAttention(d_model, num_heads, max_seq_len)\n        self.ffn = nn.Sequential(nn.Linear(d_model, dff),\n                                nn.ReLU(), nn.Linear(dff, d_model)) # Feed-forward upwards projection\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout_rate)\n    def forward(self, x:torch.Tensor, mask:torch.Tensor)->torch.Tensor:\n        \"\"\"\n       Forward pass of the encoder layer.\n\n       Args:\n           x: Input tensor of shape (B, T, d_model).\n           mask: Attention mask of shape (1, 1, T, T).\n\n       Returns:\n           Output tensor of shape (B, T, d_model).\n       \"\"\"\n        attn_output = self.self_attn(x, x, x, mask)\n        attn_output = self.dropout(attn_dropout)\n        x = self.norm1(x+attn_output)\n        ffn_output = self.ffn(x)\n        ffn_output = self.dropout(ffn_output)\n        x = self.norm2(x + ffn_output)\n        return x\n\nclass MusicTransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model:int, num_heads:int, dff:int,\n                dropout_rate:float, max_seq_len:int):\n        super().__init__()\n        self.self_attn = RelativeAttention(d_model, num_heads, max_seq_len)\n        self.cross_attn = RelativeAttention(d_model, num_heads, max_seq_len)\n        self.ffn = nn.Sequential(nn.Linear(d_model, dff),\n                                nn.ReLU(), nn.Linear(dff, d_model))\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout_rate)\n    def forward(self, x, enc_output, tgt_mask, memory_mask):\n        attn_output = self.dropout(self.self_attn(x, x, x, tgt_mask))\n        x = self.norm1(x + attn_output)\n        cross_attn_output = self.dropout(self.cross_attn(x, enc_output, enc_output,\n                                                        memory_mask))\n        x = self.norm2(x + cross_attn_output)\n        ffn_output = self.dropout(self.ffn(x))\n        return self.norm3(x + ffn_output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.1 Music Transformer Model","metadata":{}},{"cell_type":"code","source":"class MusicTransformer(nn.Module):\n    \"\"\"\n    Music Transformer model.\n\n    Args:\n        num_classes: The number of classes (vocabulary size).\n        d_model: The input/output dimension of the model.\n        num_layers: The number of encoder layers.\n        num_heads: The number of attention heads.\n        dff: The dimension of the feed-forward network.\n        dropout_rate: The dropout rate.\n        max_seq_len: The maximum sequence length.\n    \"\"\"\n    def __init__(self, num_classes:int, d_model:int,\n                num_layers:int, num_heads:int, dff:int, \n                dropout_rate:float, max_seq_len:int, pad_token:int):\n        super().__init__()\n        self.embedding = nn.Embedding(num_classes, d_model)\n        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n        self.encoder = nn.ModuleList([MusicTransformerEncoderLayer(d_model, num_heads,\n                                                                  dff, dropout_rate, \n                                                                  max_seq_len)\n                                     for _ in range(num_layers)])\n        self.decoder = nn.ModuleList([MusicTransformerDecoderLayer(d_model, num_heads,\n                                                                  dff, dropout_rate,\n                                                                  max_seq_len)\n                                     for _ in range(num_layers)])\n        self.fc = nn.Linear(d_model, num_classes)\n        self.max_seq_len = max_seq_len\n        self.pad_token = pad_token\n\n        self._init_weights() \n\n    def forward(self, src:torch.Tensor, tgt:torch.Tensor)->torch.Tensor:\n        \"\"\"\n        Forward pass of the Music Transformer.\n\n        Args:\n            x: Input tensor of shape (B, T).\n\n        Returns:\n            Output tensor of shape (B, T, num_classes).\n        \"\"\"\n        B, T_src = src.shape\n        B, T_tgt = tgt.shape\n\n        # Embedding with positional encoding\n        src_pos = torch.arange(T_src, device = src.device).unsqueeze(0).expand(B,T_src)\n        src = self.embedding(src) + self.pos_embedding(src_pos)\n\n        tgt_pos = torch.arange(T_tgt, device = tgt.device).unsqueeze(0).expand(B,T_tgt)\n        tgt = self.embedding(tgt) + self.pos_embedding(tgt_pos)\n\n        # Masks\n        src_padding_mask = (src[:,:,0] != self.pad_token).unsqueeze(1).unsqueeze(2)\n\n        tgt_padding_mask = (tgt[:,:,0] != self.pad_token).unsqueeze(1).unsqueeze(2)\n        tgt_casual_mask = torch.tril(torch.ones((T_tgt, T_tgt),\n                                               device = tgt.device)).unsqueeze(0).unsqueeze(0)\n        tgt_mask = tgt_padding_mask + tgt_casual_mask\n\n        # Encode\n        for layer in self.encoder:\n            src = layer(src, src_padding_mask)\n        # Decode\n        for layer in self.decoder:\n            tgt = layer(tgt, src, tgt_mask, src_padding_mask)\n        return self.fc(tgt)\n    \n    def _init_weights(self):\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    nn.init.zero_(module.bias)\n            elif isinstance(module, nn.LayerNorm):\n                nn.init.ones_(module.weight)\n                nn.init.zeros_(module.bias)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}